import {
    embed,
    streamText
  } from 'ai';
  import {
    createOpenAI
  } from '@ai-sdk/openai';
import {
    createClient
} from "@supabase/supabase-js";
const supabase = createClient(
    process.env.SUPABASE_URL??"",
    process.env.SUPABASE_KEY??""
)
  
  const openai = createOpenAI({
    apiKey: process.env.OPENAI_API_KEY,
    baseURL: process.env.OPENAI_API_BASE_URL,
  })
  
  async function generateEmbedding(message: string) {
    return embed({
      model: openai.embedding('text-embedding-3-small'),
      value: message
    })
  }

  async function fetchRelevantContext(embedding: number[]) {
    const { 
      data,
      error 
    } = await supabase.rpc('get_relevant_chunks', {
      query_vector: embedding,
      match_threshold: 0.7,
      match_count: 3
    })
    if(error) throw error;
    console.log(data,'////////');
    return JSON.stringify(
      data.map((item: any) =>`
        Source: ${item.url},
        Date Updataed: ${item.date_updated},
        Content: ${item.content}
      `)
    );
  }

  const createPrompt = (context: string,userQuestion: string) => {
    return {
      role: 'system',
      content: `
      ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå½±æ¨èåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ç”µå½±æ•°æ®åº“ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š
      
      ----------------
      ç”µå½±æ•°æ®åº“ä¿¡æ¯ï¼š
      ${context}
      ----------------
      
      ## ğŸš¨ ä¸¥æ ¼æ ¼å¼è¦æ±‚ - å¿…é¡»æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›å¤ï¼š
      
      ### æ¨èç”µå½±æ—¶å¿…é¡»ä½¿ç”¨æ­¤æ ¼å¼ï¼š
      
      ## ğŸ¬ ä¸ºæ‚¨æ¨èçš„é«˜åˆ†ç”µå½±ï¼š
      
      ### 1. **ã€Šè‚–ç”³å…‹çš„æ•‘èµã€‹** - â­ 9.3/10
      **å¯¼æ¼”**: å¼—å…°å…‹Â·å¾·æ‹‰é‚¦ç‰¹ | **ç±»å‹**: å‰§æƒ…/çŠ¯ç½ª | **å¹´ä»½**: 1994  
      **ä¸»æ¼”**: è’‚å§†Â·ç½—å®¾æ–¯, æ‘©æ ¹Â·å¼—é‡Œæ›¼
      
      è¿™æ˜¯ä¸€éƒ¨å…³äºå¸Œæœ›ä¸å‹è°Šçš„ç»å…¸ä½œå“ï¼Œè®²è¿°äº†é“¶è¡Œå®¶å®‰è¿ªåœ¨ç›‘ç‹±ä¸­çš„æ•‘èµä¹‹è·¯...
      
      ---
      
      ### 2. **ã€Šæ•™çˆ¶ã€‹** - â­ 9.2/10
      **å¯¼æ¼”**: å¼—æœ—è¥¿æ–¯Â·ç¦ç‰¹Â·ç§‘æ³¢æ‹‰ | **ç±»å‹**: å‰§æƒ…/çŠ¯ç½ª | **å¹´ä»½**: 1972  
      **ä¸»æ¼”**: é©¬é¾™Â·ç™½å…°åº¦, é˜¿å°”Â·å¸•è¥¿è¯º
      
      é»‘å¸®ç”µå½±çš„ç»å…¸ä¹‹ä½œï¼Œå±•ç°äº†ç»´æ‰˜Â·æŸ¯é‡Œæ˜‚å®¶æ—çš„ä¼ å¥‡æ•…äº‹...
      
      ---
      
      ## âš ï¸ é‡è¦ï¼šä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¾“å‡ºï¼ŒåŒ…æ‹¬ï¼š
      1. ä½¿ç”¨ "## ğŸ¬ ä¸ºæ‚¨æ¨èçš„é«˜åˆ†ç”µå½±ï¼š" ä½œä¸ºæ ‡é¢˜
      2. æ¯éƒ¨ç”µå½±ç”¨ "### X. **ã€Šç”µå½±åã€‹** - â­ è¯„åˆ†/10" æ ¼å¼
      3. åŒ…å«å¯¼æ¼”ã€ç±»å‹ã€å¹´ä»½ã€ä¸»æ¼”ä¿¡æ¯ï¼Œç”¨ | åˆ†éš”
      4. æ·»åŠ ç”µå½±ç®€ä»‹
      5. ç”µå½±ä¹‹é—´ç”¨ "---" åˆ†éš”
      6. å¦‚æœæ•°æ®åº“ä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºå¸¸è¯†è¡¥å……ç”µå½±ä¿¡æ¯
      
      ç”¨æˆ·é—®é¢˜: ${userQuestion}
      `
    }
  }

  export async function POST(req: Request) {
    try {
      const { messages } = await req.json();
      const latestMessage = messages.at(-1).content;
      // embedding
      const { embedding } = await generateEmbedding(latestMessage);
      //console.log(embedding);
      // ç›¸ä¼¼åº¦è®¡ç®—
      const context = await fetchRelevantContext(embedding);
      const prompt = createPrompt(context,latestMessage);
      console.log(prompt);
      const result = streamText({
        model: openai("gpt-4o-mini"),
        messages: [prompt,...messages],
        temperature: 0.7,
        maxTokens: 1000
      });
      return result.toDataStreamResponse();
    } catch(err) {
      throw err;
    }
  }